# 第 58 章 非负矩阵分解 (NMF)

<div class="context-flow" markdown>

**前置**：非负矩阵 (Ch17) · 奇异值分解 (Ch11) · 矩阵分析 (Ch14)

**本章脉络**：NMF 的动机（数据的非负本质） $\to$ 非负矩阵分解 (Non-negative Matrix Factorization) 的定义 $\to$ 非负秩 (Non-negative Rank) $r_{nn}$ 的概念 $\to$ 与 SVD 的区别：从全局正交到局部非负 $\to$ 核心特性：部分-整体 (Part-based) 表示 $\to$ 算法实现：乘性更新规则 (Multiplicative Update) $\to$ 目标函数：Frobenius 距离与 KL 散度 $\to$ 应用：文本挖掘（主题模型）、人脸识别（特征检测）、高光谱解混、推荐系统

**延伸**：NMF 是线性代数向“可解释性”妥协的产物；它通过放弃正交性而追求非负性，揭示了复杂数据如何由简单的、具有物理意义的正向分量通过“叠加”而非“抵消”而构成，是现代无监督学习的基石

</div>

在传统的 SVD 分解中，基向量可能包含负值，这在处理图像像素或词频统计时往往缺乏直观意义。**非负矩阵分解**（Non-negative Matrix Factorization, NMF）通过施加强大的非负约束，强制要求系统只能通过“加法”来合成数据。这种“只增不减”的逻辑意外地导致了**部分-整体**（Part-based）的表示方式，使我们能从数据中自动提取出具有实际含义的“零部件”。本章将探讨这一兼具计算挑战与解释能力的分解技术。

---

## 58.1 NMF 的定义与非负秩

!!! definition "定义 58.1 (非负矩阵分解)"
    给定非负矩阵 $V \in \mathbb{R}^{m \times n}$，$V \ge 0$。寻找非负矩阵 $W \in \mathbb{R}^{m \times k}$ 和 $H \in \mathbb{R}^{k \times n}$，使得：
    $$V \approx WH, \quad W, H \ge 0$$
    - $W$ 通常称为**基矩阵**（特征）。
    - $H$ 通常称为**系数矩阵**（权重）。

!!! definition "定义 58.2 (非负秩 $rank_{nn}(V)$)"
    使得 $V = WH$ 精确成立的最小中间维度 $k$ 称为 $V$ 的**非负秩**。
    **性质**：$rank(V) \le rank_{nn}(V)$。计算非负秩是 NP-难的。

---

## 58.2 核心特性：部分-整体表示

!!! technique "解释：为什么能提取“零件”？"
    由于 $W$ 和 $H$ 的元素都是非负的，数据点 $V$ 只能由基向量进行**线性叠加**而得到。为了重构出复杂的模式，算法倾向于让基向量 $W$ 稀疏化，分别代表物体的不同局部（如人脸的眼睛、鼻子）。而在 SVD 中，基向量可以通过正负抵消来拟合，因此往往产生毫无物理意义的全局重叠模式。

---

## 58.3 算法：乘性更新 (Multiplicative Update)

!!! algorithm "算法 58.1 (Lee-Seung 乘性更新)"
    针对 Frobenius 范数目标函数，更新规则如下：
    $$H_{aj} \leftarrow H_{aj} \frac{(W^T V)_{aj}}{(W^T WH)_{aj}}, \quad W_{ia} \leftarrow W_{ia} \frac{(VH^T)_{ia}}{(WHH^T)_{ia}}$$
    **特点**：只要初始值大于 0，更新过程会自动保持非负性，且在每一步降低目标函数值。

---

## 练习题

**1. [基础] 若 $V = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$，求其非负秩 $rank_{nn}(V)$。**

??? success "参考答案"
    **分析：**
    1. 该矩阵秩为 2。
    2. 由于是一个对角阵，且元素已经是 1。
    3. 我们无法用 1 个非负列向量合成这两个正交的方向。
    **结论**：非负秩等于 2。对于单位阵，非负秩与普通秩相等。

**2. [对比] 简述 NMF 与 SVD 在基向量上的主要区别。**

??? success "参考答案"
    **对比：**
    - **SVD**：基向量之间是相互正交的，且包含正负值。适合最大化方差解释，但不具有可解释性（如出现负的像素值）。
    - **NMF**：基向量不要求正交，但必须全为非负。产生“局部”特征，具有极强的物理可解释性。

**3. [计算] 验证：若 $V = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$，其非负秩是多少？给出其 NMF 分解。**

??? success "参考答案"
    **构造：**
    1. 秩为 1。
    2. $V = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \begin{pmatrix} 1 & 1 \end{pmatrix}$。
    3. 因子 $\begin{pmatrix} 1 \\ 1 \end{pmatrix} \ge 0$ 且 $\begin{pmatrix} 1 & 1 \end{pmatrix} \ge 0$。
    **结论**：非负秩为 1。

**4. [存在性] 每一个非负矩阵是否一定存在非负秩等于其普通秩的分解？**

??? success "参考答案"
    **结论：不一定。**
    这是 NMF 理论中最深刻的问题之一。存在许多矩阵（如某些特定构造的 $5 \times 5$ 阵），其普通秩为 3，但任何非负分解的中间维度 $k$ 必须至少为 4 或 5。这种“秩的鸿沟”反映了非负约束带来的代数刚性。

**5. [目标函数] 除了 Frobenius 范数，NMF 还有什么常用的损失函数？**

??? success "参考答案"
    **结论：KL 散度（Kullback-Leibler Divergence）。**
    $$D(V || WH) = \sum (V_{ij} \log \frac{V_{ij}}{(WH)_{ij}} - V_{ij} + (WH)_{ij})$$
    这在处理具有泊松噪声的数据（如计数数据、文本词频）时表现更好。

**6. [唯一性] NMF 的分解结果是唯一的吗？**

??? success "参考答案"
    **结论：通常不唯一。**
    **理由**：如果 $V = WH$，那么对于任何对角阵 $D \succ 0$，都有 $V = (WD)(D^{-1}H)$。此外，还可能存在更复杂的旋转不确定性。为了获得唯一解，通常需要施加额外的约束，如**稀疏性约束**。

**7. [应用] 在文本挖掘中，NMF 的基矩阵 $W$ 代表什么？**

??? success "参考答案"
    **解释：**
    在词项-文档矩阵中，$W$ 的每一列代表一个**主题**（Topic）。该列中数值较大的行对应的词即为该主题的核心关键词。系数矩阵 $H$ 则代表每篇文档在这些主题上的分布权重。

**8. [稀疏性] 为什么 NMF 的结果往往是稀疏的？**

??? success "参考答案"
    **代数直观：**
    由于所有元素都只能相加，为了重构出一个包含很多 0 的数据矩阵，$W$ 和 $H$ 必须在相应的位置也包含很多 0。非负性约束将解推向了非负象限的“边界”（即坐标轴），从而产生了自然的稀疏性。

**9. [复杂度] 为什么求解 NMF 是一个非凸优化问题？**

??? success "参考答案"
    **理由：**
    虽然目标函数对 $W$（固定 $H$ 时）或对 $H$（固定 $W$ 时）分别是凸的，但关于变量对 $(W, H)$ **联合非凸**。这意味着算法可能会陷入局部极小值，初始化的选择对结果有显著影响。

**10. [应用] 简述 NMF 在推荐系统（如电影评分预测）中的逻辑。**

??? success "参考答案"
    1. $V$ 是用户-电影评分矩阵。
    2. $W$ 代表用户的“兴趣偏好”（如对科幻、动作、爱情的喜爱程度）。
    3. $H$ 代表电影的“属性标签”（如电影属于哪些类别的程度）。
    4. 预测评分即为两个非负向量的内积。非负性保证了兴趣与属性的叠加是累积的，符合直觉。

## 本章小结

非负矩阵分解是线性代数在数据科学中的“可解释化”革命：

1.  **约束的奖赏**：它证明了通过施加看似限制性的非负约束，我们可以获得超越纯数学优化、具有真实物理含义的结构化分解。
2.  **局部与整体**：NMF 成功实现了“部分-整体”分解，将复杂的信号降解为原子化的组件，确立了特征提取的新范式。
3.  **计算的挑战**：非负秩的计算难度与非凸性说明了“结构解释”是有代价的，这也驱动了现代随机化和交替优化算法的不断演进。
