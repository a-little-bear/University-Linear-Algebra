# 第 29 章 线性代数在统计与机器学习中的应用

<div class="context-flow" markdown>

**前置**：奇异值分解 (Ch11) · 矩阵微积分 (Ch47A) · 投影与最小二乘 (Ch07)

**本章脉络**：数据即矩阵 $\to$ 数据预处理：去均值与单位化 $\to$ 统计量的矩阵表达：协方差矩阵 $\Sigma$ $\to$ 核心降维技术：主成分分析 (PCA) 的 SVD 推导 $\to$ 回归分析：普通最小二乘 (OLS) 的投影视角 $\to$ 线性分类：SVM 与逻辑回归中的超平面几何 $\to$ 正则化：岭回归 (Ridge) 的岭参数代数意义 $\to$ 应用：特征提取、降噪、潜在语义分析 (LSA)

**延伸**：线性代数是机器学习的“底层架构”；它将繁琐的数据点转化为高维空间的几何分布，证明了学习的过程本质上是在寻找最优投影子空间和分离超平面，是理解所有深度学习算法的数学通项

</div>

在机器学习中，每一个样本都是一个向量，每一个特征都是一个维度。**线性代数**将这些零散的数据点编织成矩阵，从而允许我们利用空间几何的直觉来寻找模式。无论是通过 **PCA** 压缩信息，还是通过**最小二乘法**预测趋势，其底层逻辑都是在寻找向量空间中的最优投影。本章将介绍这一作为 AI 引擎的代数框架。

---

## 29.1 数据矩阵与协方差

!!! definition "定义 29.1 (中心化数据矩阵 $X$)"
    设 $n$ 个样本，$d$ 个特征。减去每列均值后的矩阵 $X \in \mathbb{R}^{n \times d}$ 称为中心化矩阵。
    **协方差矩阵**：$\Sigma = \frac{1}{n-1} X^T X$。
    它描述了特征间的线性相关性。

---

## 29.2 主成分分析 (PCA)

!!! technique "技术：PCA 的 SVD 实现"
    为了对数据降维并保留最大方差：
    1.  对中心化矩阵 $X$ 进行 SVD：$X = U \Sigma V^T$。
    2.  右奇异向量 $V$ 的列即为主成分方向。
    3.  特征值（奇异值的平方）代表了沿该方向的数据方差。
    **意义**：PCA 本质上是寻找数据分布的“长轴”方向。

---

## 29.3 线性回归与正则化

!!! theorem "定理 29.1 (正规方程)"
    最小二乘解 $\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}$ 是目标向量 $\mathbf{y}$ 在 $X$ 的列空间上的正交投影。
    当 $X^T X$ 病态时，引入岭回归：$\mathbf{w} = (X^T X + \lambda I)^{-1} X^T \mathbf{y}$。

---

## 练习题

**1. [基础] 设特征 1 的方差为 4，特征 2 的方差为 9，协方差为 3。写出 $2 \times 2$ 协方差矩阵。**

??? success "参考答案"
    **构造：**
    1. 对角元为各自方差：$a_{11}=4, a_{22}=9$。
    2. 非对角元为协方差：$a_{12}=a_{21}=3$。
    **结论**：$\Sigma = \begin{pmatrix} 4 & 3 \\ 3 & 9 \end{pmatrix}$。

**2. [PCA] 在 PCA 中，为什么我们要保留特征值最大的前 $k$ 个方向？**

??? success "参考答案"
    **代数解释：**
    1. 特征值代表了数据在该方向上的**方差**。
    2. 方差越大，代表数据在该维度的区分度越高，包含的信息量越大。
    3. 舍弃小特征值方向相当于去除了低能量的噪声或不重要的细节，实现了最优的信息压缩（见 Ch11 最佳低秩逼近）。

**3. [计算] 给定数据点 $(1, 1)^T$ 和 $(-1, -1)^T$。计算其协方差矩阵。**

??? success "参考答案"
    **步骤：**
    1. 均值为 $(0, 0)^T$。数据已中心化。
    2. 矩阵 $X = \begin{pmatrix} 1 & 1 \\ -1 & -1 \end{pmatrix}$。
    3. $X^T X = \begin{pmatrix} 1 & -1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ -1 & -1 \end{pmatrix} = \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix}$。
    4. $\Sigma = \frac{1}{2-1} \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix} = \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix}$。

**4. [几何] 线性分类器的决策边界 $w^T x + b = 0$ 在几何上代表什么？**

??? success "参考答案"
    **结论：一个超平面。**
    向量 $w$ 是该超平面的**法向量**，决定了边界的方向。偏置 $b$ 决定了超平面距离原点的位移。分类过程本质上是判定点 $x$ 落在超平面的哪一侧。

**5. [逻辑回归] 简述逻辑回归梯度下降更新公式中矩阵乘法的来源。**

??? success "参考答案"
    逻辑回归的损失函数对权值 $w$ 的偏导数可以写为 $\nabla J(w) = X^T (\mathbf{p} - \mathbf{y})$。
    这里 $X^T$ 是特征矩阵的转置，$(\mathbf{p} - \mathbf{y})$ 是预测误差向量。这一矩阵-向量乘法一次性计算了所有参数的修正方向，是并行化计算的基础。

**6. [正则化] 为什么在岭回归中加入 $\lambda I$ 可以解决 $X^T X$ 不可逆的问题？**

??? success "参考答案"
    **代数理由：**
    1. $X^T X$ 是半正定的，特征值 $\ge 0$。若列相关，则最小特征值为 0。
    2. $X^T X + \lambda I$ 的特征值变为 $\lambda_i + \lambda$。
    3. 只要 $\lambda > 0$，所有特征值都严格大于 0。
    **结论**：矩阵变为严格正定，从而保证了逆矩阵的存在性与数值计算的稳定性。

**7. [应用] 什么是线性神经网络中的“全连接层”？**

??? success "参考答案"
    全连接层本质上就是一个**矩阵乘法** $y = Wx + b$。
    矩阵 $W$ 的每一行代表一个神经元的权重向量。线性代数证明了神经网络的每一层都在进行空间的线性变换（旋转、缩放、切变）。

**8. [计算] 若数据矩阵 $X$ 的奇异值为 $\{10, 5, 0.1\}$，计算前两个主成分解释的方差比例。**

??? success "参考答案"
    **计算步骤：**
    1. 总方差正比于奇异值的平方和：$10^2 + 5^2 + 0.1^2 = 100 + 25 + 0.01 = 125.01$。
    2. 前两个分量的方差贡献：$100 + 25 = 125$。
    3. 比例：$125 / 125.01 \approx 99.99\%$。
    **结论**：前两个主成分几乎保留了所有信息。

**9. [对数几率] 证明逻辑回归的损失函数是凸的。**

??? success "参考答案"
    **证明要点：**
    计算损失函数的 Hessian 矩阵，发现其形式为 $X^T D X$，其中 $D$ 是元素为 $p_i(1-p_i)$ 的正对角阵。由于这是 Gram 矩阵形式，Hessian 必半正定。根据凸性判据（见 Ch64B），该优化问题具有唯一的全局最优解。

**10. [应用] 简述 SVD 在推荐系统（潜在语义分析）中的角色。**

??? success "参考答案"
    通过对“用户-商品”交互矩阵进行截断 SVD，$X \approx U_k \Sigma_k V_k^T$。
    $U_k$ 的列提取了用户的隐含偏好，$V_k$ 的列提取了商品的隐含特征。这种低秩分解实现了特征的自动合并，解决了数据稀疏性问题。

## 本章小结

线性代数是现代智能算法的“几何骨架”：

1.  **空间的压缩**：通过 PCA 与 SVD，线性代数证明了高维数据往往坍缩在极低维的线性流形上，确立了特征提取的数学标准。
2.  **误差的投影**：从最小二乘到逻辑回归，学习的过程被统一为寻找距离目标向量最近（投影）或分离类别最清（超平面）的代数解。
3.  **计算的加速**：矩阵化的符号表达不仅简化了推导，更通过 BLAS/LAPACK 等高性能库，将机器学习的训练转化为极速的并行矩阵运算，支撑了现代大模型的运行。
