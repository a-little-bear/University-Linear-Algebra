# 第 22 章 数值线性代数

<div class="context-flow" markdown>

**前置**：矩阵分解(Ch10) · 范数与扰动(Ch15) · 线性方程组(Ch1)

**本章脉络**：舍入误差与浮点运算 → 计算复杂度 ($O(n^3)$) → 直接法（LU, QR, Cholesky）的稳定性 → 迭代法（Jacobi, Gauss-Seidel, SOR） → Krylov 子空间方法 (GMRES, 共轭梯度法 CG) → 预条件技术 (Preconditioning) → 稀疏矩阵计算

**延伸**：数值线性代数是所有高性能科学计算（气象预报、有限元分析、神经网络训练）的底层引擎

</div>

数值线性代数研究如何在有限精度的计算机上高效、稳定地解决线性代数问题。它关注的不是理论上的解，而是算法的**执行时间**、**内存效率**以及**误差控制**。

---

## 22.1 稳定性与复杂度

!!! definition "定义 22.1 (数值稳定性)"
    如果一个算法在存在舍入误差的情况下，其产生的解等价于某个稍受扰动的问题的精确解，则称该算法是**后向稳定的**。

!!! theorem "定理 22.3 (共轭梯度法收敛性)"
    对于对称正定矩阵 $A$，CG 方法在不计舍入误差的情况下在 $n$ 步内收敛。其收敛速度取决于条件数 $\sqrt{\kappa(A)}$。

---

## 练习题

1. **[复杂度] 计算 $n$ 阶矩阵相乘的理论时间复杂度是多少？Strassen 算法如何优化它？**
   ??? success "参考答案"
       标准乘法为 $O(n^3)$。Strassen 算法通过巧妙的分块减少了乘法次数，将复杂度降低至约 $O(n^{2.81})$。在实际大规模应用中，还有复杂度更低的算法（如 Coppersmith-Winograd）。

2. **[舍入误差] 为什么计算 $1.000001 - 1.000000$ 会导致精度损失？**
   ??? success "参考答案"
       这是**灾难性抵消 (Catastrophic Cancellation)**。两个接近的数相减，导致高位的有效数字被抵消，结果的有效位数大幅减少，相对误差剧增。

3. **[迭代法] 判定 Jacobi 迭代法收敛的充分条件。**
   ??? success "参考答案"
       矩阵 $A$ 是**严格对角占优**的（即 $|a_{ii}| > \sum_{j 
eq i} |a_{ij}|$）。在这种情况下，迭代矩阵的谱半径 $ho(B) < 1$。

4. **[预条件] 为什么预条件子 $M$ 选为 $A$ 的近似矩阵？**
   ??? success "参考答案"
       预条件旨在求解 $M^{-1}Ax = M^{-1}b$。若 $M \approx A$，则 $M^{-1}A \approx I$。单位阵的条件数为 1，且特征值集中，这能极大地加快 Krylov 子空间方法（如 CG 或 GMRES）的收敛速度。

5. **[QR算法] 简单描述计算矩阵全部特征值的 QR 算法流程。**
   ??? success "参考答案"
       1. 令 $A_0 = A$。
       2. 对 $A_k$ 进行 QR 分解：$A_k = Q_k R_k$。
       3. 形成下一代：$A_{k+1} = R_k Q_k$。
       重复直至 $A_k$ 收敛为上三角阵（对角线即为特征值）。通常会结合位移（Shifts）来加速收敛。

6. **[计算] 对 $A = \begin{pmatrix} 10 & 1 \ 1 & 10 \end{pmatrix}$ 进行一步 Jacobi 迭代（初始值 $x_0=0, b=(11, 11)^T$）。**
   ??? success "参考答案"
       $x_1^{(1)} = (11 - 1(0))/10 = 1.1$。
       $x_1^{(2)} = (11 - 1(0))/10 = 1.1$。
       故 $x_1 = (1.1, 1.1)^T$。

7. **[Krylov子空间] 写出 $n$ 阶 Krylov 子空间 $\mathcal{K}_n(A, b)$ 的定义。**
   ??? success "参考答案"
       $\mathcal{K}_n(A, b) = \operatorname{span}\{b, Ab, A^2b, \dots, A^{n-1}b\}$。这是迭代法（如 Lanczos, Arnoldi）搜索解的基。

8. **[稀疏矩阵] 对于大规模稀疏矩阵，为什么不直接计算 $A^{-1}$？**
   ??? success "参考答案"
       稀疏矩阵的逆矩阵通常是**稠密**的（填充现象 Fill-in）。存储 $A^{-1}$ 会导致内存溢出，且计算成本远高于直接法或迭代法。

9. **[Householder] 在 QR 分解中，Householder 变换比 Gram-Schmidt 算法有什么优势？**
   ??? success "参考答案"
       Householder 变换具有更好的**数值正交性**。修正的 Gram-Schmidt (MGS) 虽然比标准 GS 好，但在病态情况下仍可能失去正交性，而 Householder 是无条件后向稳定的。

10. **[应用] 为什么在深度学习中常用随机梯度下降 (SGD) 而非二阶牛顿法？**
    ??? success "参考答案"
        牛顿法需要存储和求逆 Hessian 矩阵，其空间复杂度为 $O(n^2)$，计算复杂度为 $O(n^3)$。对于数百万维的参数空间，数值线性代数上的代价是不可接受的。SGD 只需 $O(n)$ 复杂度。

## 本章小结

数值线性代数是计算的物理边界：

1. **误差是第一公民**：算法的优劣不再由解析正确性判定，而由误差积累速度判定。
2. **结构化计算**：针对稀疏、对称或带状结构的特化算法是高性能计算的基石。
3. **迭代的艺术**：在超大规模系统中，用有限次的局部更新替代全局精确计算是唯一的出路。
