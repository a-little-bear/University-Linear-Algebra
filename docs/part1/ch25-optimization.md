# 第 25 章 线性代数在最优化中的应用

<div class="context-flow" markdown>

**前置**：正定矩阵(Ch16) · 矩阵微积分(Ch47) · 奇异值分解(Ch11) · 矩阵流形(Ch24)

**本章脉络**：最优化问题的线性代数建模 → 无约束优化（梯度下降、牛顿法） → 线性规划 (LP) 与单纯形法 → 二次规划 (QP) → 半定规划 (SDP) → 内点法 → 拉格朗日乘子法与对偶性 → 压缩感知与 $L_1$ 最小化 → 交替方向乘子法 (ADMM)

**延伸**：最优化是线性代数理论的最大的“买家”；从搜索方向的确定到收敛性的证明，线性代数提供了全部的解析工具

</div>

最优化是寻找满足约束条件的函数极值点的过程。在这个领域，矩阵不仅是存储系数的容器，更是定义几何曲率、梯度方向和变量耦合的核心算子。几乎现代所有的复杂优化算法，其底层都是在不停地求解线性方程组或特征值问题。

---

## 25.1 核心模型与对偶理论

!!! definition "定义 25.1 (半定规划 SDP)"
    半定规划是在线性约束下，最小化线性函数，且要求变量矩阵 $X$ 是半正定的：
    $$\min \operatorname{tr}(CX), 	ext{ s.t. } \operatorname{tr}(A_i X) = b_i, X \succeq 0$$

!!! theorem "定理 25.3 (强对偶性)"
    若原问题和对偶问题都是严格可行的，则原问题的最优值等于对偶问题的最优值。这为验证解的最优性提供了理论基础。

---

## 练习题

1. **[牛顿法] 为什么牛顿法比梯度下降法收敛快？从线性代数角度解释。**
   ??? success "参考答案"
       梯度下降仅使用一阶信息（方向），而牛顿法利用了二阶信息（Hessian 矩阵的逆 $H^{-1}$）。$H^{-1}$ 起到了“预条件”的作用，它根据空间的局部曲率对梯度方向进行了修正，使得在病态（长条形）曲面上能够直接指向极小值点。

2. **[二次规划] 写出无约束二次型 $f(x) = \frac{1}{2}x^T Ax - b^T x$ 的最小值点公式。**
   ??? success "参考答案"
       求导得 $
abla f = Ax - b = 0$。若 $A \succ 0$，最小值点为 $x^* = A^{-1} b$。这说明求解线性方程组本质上是在最小化一个二次能量函数。

3. **[拉格朗日乘子] 求解 $\min \|x\|^2$ 且满足 $Ax=b$。**
   ??? success "参考答案"
       构造 $L(x, \lambda) = x^T x + \lambda^T(Ax-b)$。
       $
abla_x L = 2x + A^T \lambda = 0 \implies x = -0.5 A^T \lambda$。
       代入约束：$A(-0.5 A^T \lambda) = b \implies \lambda = -2(AA^T)^{-1}b$。
       故 $x^* = A^T(AA^T)^{-1}b$。这就是所谓的**最小范数解**（利用广义逆）。

4. **[SDP应用] 为什么半定规划在组合优化中很有用？**
   ??? success "参考答案"
       因为许多硬性的离散约束（如 $x_i \in \{-1, 1\}$）可以松弛为矩阵秩的约束，进而松弛为凸的半正定约束 $X \succeq 0$。SDP 提供了一个强大的多项式时间可解的下界估计（如 Max-Cut 问题）。

5. **[KKT条件] 写出带约束优化问题的 KKT 条件中的互补松弛性。**
   ??? success "参考答案"
       $\lambda_i g_i(x^*) = 0$。这意味着要么约束是不激活的（$\lambda_i=0$），要么解恰好落在边界上（$g_i(x^*)=0$）。

6. **[条件数] 为什么矩阵 $A$ 的条件数大时，优化算法很难收敛？**
   ??? success "参考答案"
       条件数大意味着目标函数的等高线是极其扁平的椭球。梯度方向几乎与指向极小值的方向正交，导致算法在“山谷”两侧反复震荡，步长难以确定。

7. **[ADMM] 简述交替方向乘子法 (ADMM) 的核心思想。**
   ??? success "参考答案"
       ADMM 通过分解增广拉格朗日函数，将大的耦合问题拆分为多个小的、易于求解的子问题（交替优化变量）。它特别适合处理大规模、分布式且带有非平滑项（如 $L_1$ 范数）的优化。

8. **[压缩感知] 为什么 $L_1$ 最小化能产生稀疏解？**
   ??? success "参考答案"
       $L_1$ 范数的等高线（菱形）在坐标轴上有“尖角”。当优化过程寻找与线性约束平面的切点时，极大概率切在轴上，从而强制许多分量为零。

9. **[对角占优应用] 为什么在数值优化中常用 $A + \lambda I$ 来替代不稳定的 $A$？**
   ??? success "参考答案"
       这被称为 **Tikhonov 正则化** 或岭回归。加上 $\lambda I$（$\lambda > 0$）可以强制矩阵变为严格对角占优，提高其条件数，并确保逆矩阵存在且数值稳定。

10. **[SVD与PCA] 证明主成分分析 (PCA) 本质上是一个轨迹最大化的优化问题。**
    ??? success "参考答案"
        PCA 旨在寻找投影矩阵 $P$ 使得 $\operatorname{tr}(P^T \Sigma P)$ 最大。根据 Ky Fan 最大值原理，最优的 $P$ 由协方差矩阵 $\Sigma$ 的前 $k$ 个最大特征值对应的特征向量组成。

## 本章小结

最优化是线性代数的“行动方案”：

1. **方程与极值的对偶**：解方程是求极值，求极值是解方程。
2. **曲率的语言**：Hessian 矩阵的谱属性决定了搜索的效率和解的稳定性。
3. **松弛的艺术**：将非凸、离散的问题通过矩阵化的手段嵌入到凸的半正定空间中，是现代优化算法的精髓。
