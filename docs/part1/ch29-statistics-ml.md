# 第 29 章 线性代数在统计与机器学习中的应用

<div class="context-flow" markdown>

**前置**：SVD(Ch11) · 矩阵分析(Ch14) · 正定矩阵(Ch16) · 最优化(Ch25)

**本章脉络**：数据矩阵与中心化 → 协方差矩阵与特征分解 → 主成分分析 (PCA) → 线性判别分析 (LDA) → 最小二乘与投影算子 → 岭回归 (Ridge Regression) 与吉洪诺夫正则化 → 神经网络中的权重矩阵与反向传播 → 核方法与正定核 → 降维理论初步

**延伸**：机器学习是高维空间的几何学；几乎所有的主流算法都可以看作是在进行特定约束下的矩阵分解或投影

</div>

统计学与机器学习研究如何从数据中提取结构。在线性代数的视角下，特征提取是子空间投影，降维是低秩逼近，而模型训练则是大规模矩阵参数的优化。

---

## 29.1 数据表示与核心算法

!!! definition "定义 29.1 (样本协方差矩阵)"
    设 $X$ 为 $n 	imes d$ 的中心化数据矩阵，其样本协方差矩阵为 $S = \frac{1}{n-1} X^T X$。这是一个半正定对称阵。

!!! theorem "定理 29.3 (PCA 与特征分解)"
    PCA 的前 $k$ 个主成分方向对应于协方差矩阵 $S$ 的前 $k$ 个最大特征值所关联的特征向量。

---

## 练习题

1. **[数据中心化] 给定数据矩阵 $X = \begin{pmatrix} 1 & 2 \ 3 & 4 \end{pmatrix}$，计算其中心化矩阵。**
   ??? success "参考答案"
       均值向量 $\mu = [(1+3)/2, (2+4)/2] = [2, 3]$。
       中心化矩阵 $X_c = X - \mathbf{1}\mu = \begin{pmatrix} 1-2 & 2-3 \ 3-2 & 4-3 \end{pmatrix} = \begin{pmatrix} -1 & -1 \ 1 & 1 \end{pmatrix}$。

2. **[PCA与方差] 为什么 PCA 选择特征值最大的方向？**
   ??? success "参考答案"
       特征值代表了数据在该特征向量方向上的方差。为了在降维时尽可能保留原始数据的信息（即保持多样性），我们需要选择方差最大的方向进行投影。

3. **[最小二乘] 证明：最小二乘解 $\hat{\beta} = (X^T X)^{-1} X^T y$ 是 $y$ 在 $X$ 的列空间上的投影系数。**
   ??? success "参考答案"
       投影向量 $p = X\hat{\beta} = X(X^T X)^{-1} X^T y = P_X y$。其中 $P_X = X(X^T X)^{-1} X^T$ 恰好是列空间的正交投影矩阵。最小化误差等价于寻找投影点。

4. **[岭回归] 在岭回归中，正则化项 $\lambda I$ 对协方差矩阵 $X^T X$ 的特征值有何影响？**
   ??? success "参考答案"
       它将每个特征值 $\lambda_i$ 增加 $\lambda$。这提高了矩阵的条件数，解决了当数据矩阵接近亏秩时逆矩阵不稳定的问题，从而提高了模型的鲁棒性。

5. **[LDA与PCA] 简单描述 PCA 与 LDA 在子空间选择上的区别。**
   ??? success "参考答案"
       - **PCA**：无监督，寻找使**总方差**最大的方向。
       - **LDA**：有监督，寻找使**类间方差最大**且**类内方差最小**的方向。

6. **[权重矩阵] 神经网络的一个全连接层 $y = \sigma(Wx + b)$ 中，权重矩阵 $W$ 起什么作用？**
   ??? success "参考答案"
       $W$ 执行从输入特征空间到输出特征空间的线性映射（旋转、缩放和降维/升维）。它是系统学习到的核心特征变换算子。

7. **[SVD与协同过滤] 在推荐系统中，如何利用 SVD 处理用户-评分矩阵？**
   ??? success "参考答案"
       通过截断 SVD 将稀疏的评分矩阵 $R$ 分解为 $U \Sigma V^T$。其中 $U$ 代表用户潜在偏好，$V$ 代表物品潜在特征。通过低秩逼近补全缺失项，从而预测用户对未见物品的评分。

8. **[核技巧] 为什么 Mercer 定理对机器学习很重要？**
   ??? success "参考答案"
       Mercer 定理保证了只要核函数满足正定性条件，它就对应于某个高维空间中的内积。这允许我们在低维空间计算高维投影（如 SVM），而无需显式构造高维向量。

9. **[计算] 求 $X = \begin{pmatrix} 1 & 1 \ 1 & 1 \end{pmatrix}$ 的主成分方向。**
   ??? success "参考答案"
       $S \propto X^T X = \begin{pmatrix} 2 & 2 \ 2 & 2 \end{pmatrix}$。特征值为 4 和 0。对应特征值为 4 的特征向量为 $[1/\sqrt{2}, 1/\sqrt{2}]^T$。这就是第一主成分。

10. **[奇异值与稳定性] 数据矩阵 $X$ 的奇异值分布如何反映其质量？**
    ??? success "参考答案"
        若奇异值迅速衰减，说明数据具有很强的低秩结构，适合降维。若所有奇异值都很大且接近，说明各维度间几乎独立，降维会损失大量信息。

## 本章小结

统计机器学习是线性代数的现代疆域：

1. **投影是核心**：所有的估计和逼近本质上都是在子空间中寻找最接近的点。
2. **谱分析是洞察**：特征值揭示了数据的主要矛盾，决定了信号与噪声的分野。
3. **正则化是约束**：通过对特征值进行人为修正，我们在精度与稳定性之间取得了代数平衡。
