# 第 11 章 奇异值分解 (SVD)

<div class="context-flow" markdown>

**前置**：矩阵分解 (Ch10) · 特征值 (Ch06) · 正交性 (Ch07) · 矩阵范数 (Ch15)

**本章脉络**：SVD 的存在性与唯一性定理 $\to$ 奇异值与奇异向量的几何意义（超椭圆变换） $\to$ 紧 SVD 与截断 SVD $\to$ 与特征值分解的深层关系（$A^* A$ 与 $AA^*$） $\to$ 最佳低秩逼近（Eckart-Young 定理） $\to$ 广义逆矩阵（Moore-Penrose 伪逆） $\to$ 统计学应用：主成分分析 (PCA) 的代数核心 $\to$ 信号处理：图像压缩与去噪

**延伸**：奇异值分解被公认为线性代数的“巅峰”；它消除了方阵与长方阵、满秩与亏秩之间的界限，为描述任何线性算子的效能提供了终极的解析工具，是现代数据科学与 AI 的底层地基

</div>

奇异值分解（Singular Value Decomposition, SVD）是对任意矩阵的终极剖析。如果说特征值分解是针对方阵的一种“内部解构”，那么 SVD 则是对任何线性映射的一种“全局评估”。它不仅揭示了矩阵的秩结构，还给出了矩阵作为算子时对空间各向异性的拉伸规律。本章将展示 SVD 如何将矩阵转化为最简捷的能量分量之和。

---

## 11.1 SVD 的定义与几何本质

!!! theorem "定理 11.1 (SVD 存在性定理)"
    对于任意 $m \times n$ 复矩阵 $A$，都存在分解：
    $$A = U \Sigma V^*$$
    其中：
    - $U \in M_m$ 是酉矩阵，其列向量称为**左奇异向量**。
    - $V \in M_n$ 是酉矩阵，其列向量称为**右奇异向量**。
    - $\Sigma \in M_{m \times n}$ 是矩形对角矩阵，对角元 $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > 0$ 称为**奇异值**。

!!! technique "几何：超椭圆变换"
    SVD 表明任何线性映射都可以分解为三步：
    1.  **旋转** ($V^*$)：将输入空间的轴旋转到主成分方向。
    2.  **伸缩** ($\Sigma$)：沿着这些轴进行拉伸或压缩（奇异值即为拉伸倍数）。
    3.  **再旋转** ($U$)：将结果旋转到输出空间的最终姿态。
    因此，$A$ 将单位球映为一个**超椭球**，奇异值对应于半轴的长度。

---

## 11.2 最佳低秩逼近

!!! theorem "定理 11.2 (Eckart-Young 定理)"
    在所有秩为 $k$ ($k < r$) 的矩阵中，使得 Frobenius 范数距离 $\|A - A_k\|_F$ 最小的矩阵是截断 SVD：
    $$A_k = \sum_{i=1}^k \sigma_i \mathbf{u}_i \mathbf{v}_i^*$$
    **应用**：这是数据压缩（仅保留大奇异值）和主成分分析 (PCA) 的数学准则。

---

## 11.3 Moore-Penrose 伪逆

!!! definition "定义 11.1 (伪逆 $A^+$)"
    利用 SVD 可以定义任何矩阵（不论是否方阵、是否满秩）的**广义逆**：
    $$A^+ = V \Sigma^+ U^*$$
    其中 $\Sigma^+$ 是将 $\Sigma$ 的非零元取倒数再转置得到的矩阵。
    **性质**：对于任何 $b$，$\hat{x} = A^+ b$ 都是线性最小二乘问题的最小范数解。

---

## 练习题

**1. [计算] 求 $A = \begin{pmatrix} 3 & 0 \\ 0 & -2 \end{pmatrix}$ 的奇异值和 SVD。**

??? success "参考答案"
    **步骤：**
    1. 奇异值必须非负。
    2. 对角元为 $3$ 和 $-2$。
    3. 奇异值为 $\sigma_1 = 3, \sigma_2 = 2$。
    4. 为了让 $\Sigma = \operatorname{diag}(3, 2)$，$U$ 需要修正第二个轴的方向。
    **SVD 分解**：
    $A = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix} \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$。
    这里 $U = \operatorname{diag}(1, -1), V = I$。

**2. [关系] 证明 $A$ 的奇异值的平方是 $A^* A$ 的特征值。**

??? success "参考答案"
    **证明：**
    1. 代入 SVD 表达式：$A^* A = (V \Sigma^T U^*)(U \Sigma V^*) = V (\Sigma^T \Sigma) V^*$。
    2. 由于 $V$ 是酉矩阵，$V^*$ 是其逆，这正是 $A^* A$ 的**特征值分解**形式。
    3. 特征值矩阵为 $\Sigma^T \Sigma = \operatorname{diag}(\sigma_1^2, \sigma_2^2, \ldots)$。
    **结论**：奇异值是 $A^* A$ 特征值的算术平方根。

**3. [秩] 若 $A$ 的奇异值为 $5, 3, 0, 0$，则 $\operatorname{rank}(A)$ 是多少？它的零空间维数是多少？**

??? success "参考答案"
    **判定：**
    1. 秩等于非零奇异值的个数：$\operatorname{rank}(A) = 2$。
    2. 零空间维数（零化度）等于列数减去秩。若 $A$ 是 $4 \times 4$ 的，则为 $4 - 2 = 2$。
    **结论**：非零奇异值揭示了矩阵传播信息的“有效维度”。

**4. [范数] 证明 $\|A\|_2 = \sigma_1$。**

??? success "参考答案"
    **证明：**
    1. 算子范数 $\|A\|_2 = \max \frac{\|Ax\|}{\|x\|}$。
    2. 代入 SVD：$\|U \Sigma V^* x\| = \|\Sigma (V^* x)\|$（因为 $U$ 保持范数）。
    3. 令 $y = V^* x$。由于 $V^*$ 酉，$\|y\| = \|x\|$。
    4. 问题变为最大化 $\|\Sigma y\|$ 且 $\|y\|=1$。
    5. 由于 $\Sigma$ 是对角阵且 $\sigma_1$ 最大，最大值显然在 $y = e_1$ 处取得，即 $\sigma_1$。

**5. [低秩逼近] 设 $A$ 的奇异值为 $10, 8, 1$。其最佳秩 2 逼近的误差（Frobenius 范数）是多少？**

??? success "参考答案"
    **解析：**
    1. 最佳秩 $k$ 逼近是通过舍弃较小的奇异值得到的。
    2. 误差 $\|A - A_k\|_F$ 等于被舍弃奇异值的平方和的平方根。
    3. 被舍弃的是 $\sigma_3 = 1$。
    **结论**：误差为 $\sqrt{1^2} = 1$。这说明绝大部分“能量”已被秩 2 矩阵捕捉。

**6. [伪逆] 若 $A = \operatorname{diag}(2, 0)$，求 $A^+$ 并验证 $AA^+ A = A$。**

??? success "参考答案"
    **计算：**
    1. 将非零元取倒数，零元保持不变。
    2. $A^+ = \operatorname{diag}(0.5, 0)$。
    **验证：**
    $AA^+ = \operatorname{diag}(1, 0)$。
    $A A^+ A = \operatorname{diag}(1, 0) \operatorname{diag}(2, 0) = \operatorname{diag}(2, 0) = A$。
    满足 Penrose 第一条件。

**7. [正交性] SVD 中的 $U$ 和 $V$ 是否唯一？**

??? success "参考答案"
    **结论：**
    **不唯一**。
    **理由**：
    1. 特征向量的正负号（或复数域的相位）可以任意改变而不影响乘积。
    2. 若存在重特征值，对应的特征向量可以在特征子空间内任意旋转。
    但**奇异值序列 $\Sigma$ 是唯一的**。

**8. [紧SVD] 什么是紧 SVD (Compact SVD)？它与完全 SVD 有什么区别？**

??? success "参考答案"
    **区别：**
    - **完全 SVD**：$U$ 是 $m \times m$，$V$ 是 $n \times n$。
    - **紧 SVD**：仅保留对应于非零奇异值的列。若秩为 $r$，则 $U_r$ 为 $m \times r$，$V_r$ 为 $n \times r$。
    **意义**：紧 SVD 剔除了对结果没有贡献的基向量，是计算中更常用的形式。

**9. [应用] 为什么 SVD 可以用于图像压缩？简述原理。**

??? success "参考答案"
    **原理：**
    1. 图像可以看作一个大的灰度值矩阵 $A$。
    2. 现实图像往往具有高度的局部相关性，意味着奇异值衰减非常快。
    3. 通过只保留前 $k$ 个最大的奇异值（截断 SVD），可以用极少量的数据重构出视觉上几乎无损的图像。
    4. 这将存储需求从 $mn$ 降到了 $(m+n)k$。

**10. [条件数] 矩阵的条件数 $\kappa(A)$ 用奇异值如何表示？**

??? success "参考答案"
    **结论：**
    对于非奇异方阵，$\kappa(A) = \sigma_{\max} / \sigma_{\min}$。
    **意义**：这说明如果一个矩阵有非常接近 0 的奇异值，它就是“病态”的，其求逆运算对噪声极其敏感。

## 本章小结

SVD 是线性代数解决实际问题的终极武器：

1.  **普适性**：它消除了方阵与长方阵、满秩与亏秩之间的理论隔阂，为所有线性映射提供了统一的解析视角。
2.  **能量集中**：通过奇异值的大小排序，SVD 识别了数据中“最重要”的成分，为信息压缩与降维提供了数学准则。
3.  **数值鲁棒性**：作为伪逆和最小二乘解的计算基础，SVD 在面对病态矩阵时表现出极高的稳定性，是工程计算的最后一道防线。
